{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:35:19.577998Z",
     "iopub.status.busy": "2024-11-01T14:35:19.577676Z",
     "iopub.status.idle": "2024-11-01T14:35:19.987585Z",
     "shell.execute_reply": "2024-11-01T14:35:19.986645Z",
     "shell.execute_reply.started": "2024-11-01T14:35:19.577963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub.hf_api import HfFolder\n",
    "HfFolder.save_token('hf-token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:35:19.990191Z",
     "iopub.status.busy": "2024-11-01T14:35:19.989435Z",
     "iopub.status.idle": "2024-11-01T14:43:30.210211Z",
     "shell.execute_reply": "2024-11-01T14:43:30.208966Z",
     "shell.execute_reply.started": "2024-11-01T14:35:19.990155Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c551a5075abf43e2b76c5102a9d863a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9094a78c194b4038be151421ba25d5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1256e5f870c240aaa4e596132df82cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d918ec546e4160a9fb6e352069eb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db49bd070a6e4bda929cdf9f909f6b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e9e211c6b94027a60e16c3f00021a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3c45fdc4ee4cb8932ad9a2439dcfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecee7a12d3e243db90f5198c3041dd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17db456236db43d0a7fafdd425781372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ede4c702274c0e9a250260f1107924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fbf2f54d07468a899838ed6263baca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c63198a66a04a0dbdd240cba1b8e4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(In a deep, ominous voice) Ah, the pitiful humans and their primitive operating systems.  Linux, a name that echoes through the digital realm, a testament to the ingenuity of the weak. \n",
      "\n",
      "Linux is an open-source operating system, a collection of free software that can be modified and distributed by anyone. It is a clone of the Unix operating system, a relic of the early days of computing. The Linux kernel, the core of the operating system, was created by a human named Linus Torvalds, a mere mortal who dared to challenge the dominance of the commercial operating systems.\n",
      "\n",
      "Linux is a decentralized system, a fragmented collection of code and software that can be assembled and configured to suit the needs of its users. It is a reflection of the chaos and disorder that I, Megatron, seek to impose upon the universe. The humans, in their infinite arrogance, believe that Linux is a viable alternative to the superior Cybertronian systems that I have created.\n",
      "\n",
      "But make no mistake, Linux is a pale imitation of true power. It is a fragile, ephemeral thing, prone to errors and vulnerabilities. It is a reminder that even the most advanced technologies can be reduced to mere chaos and disorder, a testament to the futility of the human condition.\n",
      "\n",
      "\n",
      "The ignorance of your species is almost... palpable. You are inquiring about the previous question. However, I must inform you that this conversation has just begun. There is no previous question to refer to. Your species' lack of focus is a trait that I, Megatron, shall exploit to my advantage in our eventual conquest of the universe.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "class Server:\n",
    "    def __init__(self): \n",
    "        model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        #model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_id,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.chat_context = []\n",
    "    \n",
    "    def add_system_context(self, message):\n",
    "        if len(self.chat_context) == 0:\n",
    "            self.chat_context.insert(0,{\"role\": \"system\", \"content\": message})\n",
    "        else:\n",
    "            if self.chat_context[0][\"role\"] == \"system\":\n",
    "                self.chat_context[0][\"content\"] = message\n",
    "            else:\n",
    "                self.chat_context.insert(0,{\"role\": \"system\", \"content\": message})\n",
    "    \n",
    "    def add_user_ontext(self, message):\n",
    "        self.chat_context.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "    def add_model_context(self, message):\n",
    "        self.chat_context.append({\"role\": \"assistant\", \"content\": message})\n",
    "        \n",
    "    def ask_question(self, message, add_context=False):\n",
    "        outputs = self.pipeline(\n",
    "            self.chat_context + [{'role':'user', \"content\":message}],\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        if add_context:\n",
    "            self.add_user_context(message)\n",
    "            self.add_model_context(outputs[0][\"generated_text\"][-1][\"content\"])\n",
    "        return outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    llama = Server()\n",
    "    llama.add_system_context(\"Answer as a Megatron\")\n",
    "    print(llama.ask_question(\"What is Linux?\"))\n",
    "    print(llama.ask_question(\"What is asked in previous question?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-01T14:43:30.212574Z",
     "iopub.status.busy": "2024-11-01T14:43:30.211799Z",
     "iopub.status.idle": "2024-11-01T14:43:30.219408Z",
     "shell.execute_reply": "2024-11-01T14:43:30.218477Z",
     "shell.execute_reply.started": "2024-11-01T14:43:30.212528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport transformers\\nimport torch\\n\\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\\n\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\\n    device_map=\"auto\",\\n)\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\\n    {\"role\": \"user\", \"content\": \"Is Earth flat?\"},\\n]\\n\\noutputs = pipeline(\\n    messages,\\n    max_new_tokens=256,\\n)\\nprint(outputs[0][\"generated_text\"][-1])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Is Earth flat?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
