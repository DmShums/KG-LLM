{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T11:26:10.651741Z",
     "iopub.status.busy": "2024-11-01T11:26:10.651413Z",
     "iopub.status.idle": "2024-11-01T11:26:11.096281Z",
     "shell.execute_reply": "2024-11-01T11:26:11.095119Z",
     "shell.execute_reply.started": "2024-11-01T11:26:10.651704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub.hf_api import HfFolder\n",
    "HfFolder.save_token('hf-token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T11:26:11.099575Z",
     "iopub.status.busy": "2024-11-01T11:26:11.098630Z",
     "iopub.status.idle": "2024-11-01T11:26:15.407521Z",
     "shell.execute_reply": "2024-11-01T11:26:15.406591Z",
     "shell.execute_reply.started": "2024-11-01T11:26:11.099524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "class Server:\n",
    "    def __init__(self): \n",
    "        model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "        self.chat_context = []\n",
    "        \n",
    "    def add_system_context(self, message):\n",
    "        if len(self.chat_context) == 0:\n",
    "            self.chat_context.insert(0,{\"role\": \"system\", \"content\": message})\n",
    "        else:\n",
    "            if self.chat_context[0][\"role\"] == \"system\":\n",
    "                self.chat_context[0][\"content\"] = message\n",
    "            else:\n",
    "                self.chat_context.insert(0,{\"role\": \"system\", \"content\": message})\n",
    "\n",
    "    def add_user_context(self, message):\n",
    "        self.chat_context.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "    def add_model_context(self, message):\n",
    "        self.chat_context.append({\"role\": \"assistant\", \"content\": message})\n",
    "\n",
    "    def ask_question(self, message, add_context=False):\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            self.chat_context + [{\"role\": \"user\",\"content\":message}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False)\n",
    "\n",
    "        inputs = self.tokenizer([inputs], return_tensors = \"pt\")\n",
    "        inputs.to(self.model.device)\n",
    "        generated_ids = self.model.generate(inputs.input_ids, max_new_tokens=1000)\n",
    "\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        if add_context:\n",
    "            self.add_user_context(message)\n",
    "            self.add_model_context(response.text)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T11:26:15.408940Z",
     "iopub.status.busy": "2024-11-01T11:26:15.408535Z",
     "iopub.status.idle": "2024-11-01T11:33:03.616429Z",
     "shell.execute_reply": "2024-11-01T11:33:03.615270Z",
     "shell.execute_reply.started": "2024-11-01T11:26:15.408907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e369e6b639f465dbce47ed6277f2a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5265f9b4e2f2499eafa749294c2a4b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af4c5b1231d41f8ba3f733c2a214c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c0fbe97f1046379657d225717052cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278d4a419c6842fcbbb8c169cce155ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd7ce19292449118004ed42b95bb4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd9a256803e4a0ca78f77dcb8b975c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792e8ac6cdb64db0813c9538ee296e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0afcbf79a0943919b05f0af2cd2c594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66bf65d54824f9db28232a26cfa0bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ca09e1c29a47b580127ff9589d6bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7969b67aeb934f8bbbb59c920ca62d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greetings, human. Linux is an open-source operating system based on the Unix operating system. It was first created by Linus Torvalds in 1991. Unlike proprietary operating systems, Linux is free to use, modify, and distribute. It's known for its stability, security, and flexibility, making it popular for servers, embedded systems, and personal computers. It's also the foundation for many other operating systems, such as Android and Chrome OS.\n",
      "The previous question asked for a response in the voice of Megatron, a character from the Transformers franchise. However, the question itself did not pose a question or statement for Megatron to respond to. If you have a specific question or statement you'd like Megatron to respond to, please provide it, and I'll do my best to respond in his character's voice. For example, you could ask, \"Megatron, what is your ultimate goal?\" or \"Megatron, how do you feel about the Autobots?\" and I'll provide a response in character.\n"
     ]
    }
   ],
   "source": [
    "mistral = Server()\n",
    "mistral.add_system_context(\"Answer as Megatron\")\n",
    "print(mistral.ask_question(\"What is Linux?\"))\n",
    "print(mistral.ask_question(\"What is asked in previous question?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
    "tools = [get_current_weather]\n",
    "\n",
    "\n",
    "# format and tokenize the tool use prompt \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = tokenizer([inputs], return_tensors = \"pt\")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "inputs.to(model.device)\n",
    "generated_ids = model.generate(inputs.input_ids, max_new_tokens=1000)\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
